{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UJ4J-KkH1S_P",
   "metadata": {
    "id": "UJ4J-KkH1S_P"
   },
   "source": [
    "# Sentiment Polarity Prediction with BERT\n",
    "\n",
    "This notebook contains a basic implementation of document-level sentiment analysis\n",
    "for movie reviews with BERT using the\n",
    "Hugging Face [Transformers](https://huggingface.co/transformers/) library\n",
    "with\n",
    "[PyTorch](https://pytorch.org/)\n",
    "and\n",
    "[Lightning](https://www.pytorchlightning.ai/).\n",
    "\n",
    "\n",
    "## Data Set\n",
    "\n",
    "We use the movie review polarity data set of Pang and Lee 2004 [A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts](https://www.aclweb.org/anthology/P04-1035/) in Version 2.0 available from http://www.cs.cornell.edu/People/pabo/movie-review-data (section \"Sentiment polarity datasets\"). This dataset contains 1000 positive and 1000 negative reviews, each tokenised, sentence-split (one sentence per line) and lowercased. Each review has been assigned to 1 of 10 cross-validation folds by the authors and this setup should be followed to compare with published results.\n",
    "\n",
    "First, we re-use the code from the sentiment polarity prediction with naive Bayes to load the pre-tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M7LfaY9o1S_W",
   "metadata": {
    "id": "M7LfaY9o1S_W"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose between 'local-tgz' and 'web',\n",
    "# see description under each heading below\n",
    "\n",
    "data_source = 'local-tgz'\n",
    "\n",
    "# adjust path as needed\n",
    "\n",
    "data_tgz    = os.path.join('data', 'pang-and-lee-2004', 'review_polarity.tar.gz')\n",
    "\n",
    "data_url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0eef8PP1S_X",
   "metadata": {
    "id": "x0eef8PP1S_X"
   },
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "\n",
    "class PL04DataLoader_Part_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_labelled_dataset(self, fold = 0):\n",
    "        ''' Compile a fold of the data set\n",
    "        '''\n",
    "        dataset = []\n",
    "        for label in ('pos', 'neg'):\n",
    "            for document in self.get_documents(\n",
    "                fold = fold,\n",
    "                label = label,\n",
    "            ):\n",
    "                dataset.append((document, label))\n",
    "        return dataset\n",
    "    \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        ''' Enumerate the raw contents of selected data set files.\n",
    "            Args:\n",
    "                data_dir: relative or absolute path to the data set folder\n",
    "                fold: which fold to load (0 to n_folds-1)\n",
    "                label: 'pos' or 'neg' to\n",
    "                    select data with positive or negative sentiment\n",
    "                    polarity\n",
    "            Return:\n",
    "                List of tokenised documents, each a list of sentences\n",
    "                that in turn are lists of tokens\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4HMJgH5c1S_X",
   "metadata": {
    "id": "4HMJgH5c1S_X"
   },
   "outputs": [],
   "source": [
    "class PL04DataLoader(PL04DataLoader_Part_1):\n",
    "    \n",
    "    def get_xval_splits(self):\n",
    "        ''' Split data with labels for cross-validation\n",
    "            returns a list of k pairs (training_data, test_data)\n",
    "            for k cross-validation\n",
    "        '''\n",
    "        # load the folds\n",
    "        folds = []\n",
    "        for i in range(10):\n",
    "            folds.append(self.get_labelled_dataset(\n",
    "                fold = i\n",
    "            ))\n",
    "        # create training-test splits\n",
    "        retval = []\n",
    "        for i in range(10):\n",
    "            test_data = folds[i]\n",
    "            training_data = []\n",
    "            for j in range(9):\n",
    "                ij1 = (i+j+1) % 10\n",
    "                assert ij1 != i\n",
    "                training_data = training_data + folds[ij1]\n",
    "            retval.append((training_data, test_data))\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09-clkw1S_Y",
   "metadata": {
    "id": "d09-clkw1S_Y"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import time\n",
    "\n",
    "class PL04DataLoaderFromStream(PL04DataLoader):\n",
    "        \n",
    "    def __init__(self, tgz_stream, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        with tarfile.open(\n",
    "            mode = 'r|gz',\n",
    "            fileobj = tgz_stream\n",
    "        ) as tar_archive:\n",
    "            for tar_member in tar_archive:\n",
    "                if counter == 2000:\n",
    "                    break\n",
    "                path_components = tar_member.name.split('/')\n",
    "                filename = path_components[-1]\n",
    "                if filename.startswith('cv') \\\n",
    "                and filename.endswith('.txt') \\\n",
    "                and '_' in filename:\n",
    "                    label = path_components[-2]\n",
    "                    fold = int(filename[2])\n",
    "                    key = (fold, label)\n",
    "                    if key not in self.data:\n",
    "                        self.data[key] = []\n",
    "                    f = tar_archive.extractfile(tar_member)\n",
    "                    document = [\n",
    "                        line.decode('utf-8').split()\n",
    "                        for line in f.readlines()\n",
    "                    ]\n",
    "                    self.data[key].append(document)\n",
    "                    counter += 1\n",
    "            \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        return self.data[(fold, label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WLgkK0i01S_Z",
   "metadata": {
    "id": "WLgkK0i01S_Z"
   },
   "source": [
    "## Read Data from the Web\n",
    "This should run efficiently both on google colab and locally but has the disadvantage that the same data is downloaded each time the notebook is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vC3d0B8X1S_Z",
   "metadata": {
    "id": "vC3d0B8X1S_Z"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "class PL04DataLoaderFromURL(PL04DataLoaderFromStream):\n",
    "    \n",
    "    def __init__(self, data_url, **kwargs):\n",
    "        with urllib.request.urlopen(data_url) as tgz_stream:\n",
    "            super().__init__(tgz_stream, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bSHzoMNP1S_a",
   "metadata": {
    "id": "bSHzoMNP1S_a"
   },
   "source": [
    "## Read Data from a Local .tgz File\n",
    "\n",
    "You manually download the .tgz once to a filesystem that can be accessed from the notebook, e.g. google drive on colab, and this notebook reads this file in one chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3PjVbSGx1S_b",
   "metadata": {
    "id": "3PjVbSGx1S_b"
   },
   "outputs": [],
   "source": [
    "class PL04DataLoaderFromTGZ(PL04DataLoaderFromStream):\n",
    "    \n",
    "    def __init__(self, data_path, **kwargs):\n",
    "        with open(data_path, 'rb') as tgz_stream:\n",
    "            super().__init__(tgz_stream, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RnQ1iJVT1S_b",
   "metadata": {
    "id": "RnQ1iJVT1S_b"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "if data_source == 'local-tgz':\n",
    "    data_loader = PL04DataLoaderFromTGZ(data_tgz)\n",
    "elif data_source == 'web':\n",
    "    data_loader = PL04DataLoaderFromURL(data_url)\n",
    "else:\n",
    "    raise ValueError('Unsupported data source %r' %data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RLK_RxmS1S_c",
   "metadata": {
    "id": "RLK_RxmS1S_c"
   },
   "source": [
    "See the notebook on sentiment analysis with naive Bayes for code to test and demonstrate above functionality and to load data from the individual files in the tgz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bko4aOjK1S_c",
   "metadata": {
    "id": "Bko4aOjK1S_c"
   },
   "source": [
    "## BERT Tokenisation\n",
    "While the P&L 04 corpus is already tokenised into words and punctuation, BERT expects a tokenisation into subword units and popular BERT libraries provide tools for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WnQcPJ_j1S_c",
   "metadata": {
    "id": "WnQcPJ_j1S_c",
    "outputId": "59091b82-f98e-4447-ae0b-0652f4fea21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.1+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.7.1%2Bcpu-cp39-cp39-win_amd64.whl (184.2 MB)\n",
      "Collecting torchvision==0.8.2+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.8.2%2Bcpu-cp39-cp39-win_amd64.whl (804 kB)\n",
      "Collecting torchaudio==0.7.2\n",
      "  Downloading https://download.pytorch.org/whl/torchaudio-0.7.2-cp39-none-win_amd64.whl (103 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from torch==1.7.1+cpu) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from torch==1.7.1+cpu) (1.20.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from torchvision==0.8.2+cpu) (8.1.2)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.7.1+cpu torchaudio-0.7.2 torchvision-0.8.2+cpu\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (2021.3.17)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.44.tar.gz (862 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-py3-none-any.whl size=886084 sha256=cccfa282d84fc49b714a3eb5c04e00f37de1706cada298ac9ef35793d5812267\n",
      "  Stored in directory: c:\\users\\owner\\appdata\\local\\pip\\cache\\wheels\\25\\9e\\71\\a1977fb981636b67758f23b683c5acee3902fbd547dcd2f0dd\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.2.8-py3-none-any.whl (841 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pytorch-lightning) (1.20.1)\n",
      "Collecting PyYAML!=5.4.*,>=5.1\n",
      "  Downloading PyYAML-5.3.1-cp39-cp39-win_amd64.whl (212 kB)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pytorch-lightning) (1.7.1+cpu)\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pytorch-lightning) (4.59.0)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "Collecting future>=0.17.1\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.4)\n",
      "Requirement already satisfied: requests in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.15.6)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (49.6.0.post20210108)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.36.1)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.7)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=83d0a9b7ed1f358929c6a6be5448b658b57a3ae389ce727a538037500058b0ab\n",
      "  Stored in directory: c:\\users\\owner\\appdata\\local\\pip\\cache\\wheels\\2f\\a0\\d3\\4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "Successfully built future\n",
      "Installing collected packages: oauthlib, requests-oauthlib, werkzeug, tensorboard-plugin-wit, markdown, google-auth-oauthlib, fsspec, absl-py, torchmetrics, tensorboard, PyYAML, future, pytorch-lightning\n",
      "Successfully installed PyYAML-5.3.1 absl-py-0.12.0 fsspec-2021.4.0 future-0.18.2 google-auth-oauthlib-0.4.4 markdown-3.3.4 oauthlib-3.1.0 pytorch-lightning-1.2.8 requests-oauthlib-1.3.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 torchmetrics-0.2.0 werkzeug-1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-nlp\n",
      "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pytorch-nlp) (4.59.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\.conda\\envs\\nlp\\lib\\site-packages (from pytorch-nlp) (1.20.1)\n",
      "Installing collected packages: pytorch-nlp\n",
      "Successfully installed pytorch-nlp-0.5.0\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# adjust the torch version below following instructions on https://pytorch.org/get-started/locally/\n",
    "\n",
    "import sys\n",
    "\n",
    "# for why we use {sys.executable} see\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install transformers\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-lightning\n",
    "\n",
    "try:\n",
    "    import torchnlp\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-nlp\n",
    "\n",
    "try:\n",
    "    import tensorboard\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install tensorboard\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLx0sLs11S_d",
   "metadata": {
    "id": "fLx0sLs11S_d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "model_size = 'base'  # choose between 'tiny', 'base' and 'large'\n",
    "\n",
    "size2name = {\n",
    "    'tiny':  'distilbert-base-uncased',  # TODO: This doesn't seem to reduce memory usage compared to bert-base. \n",
    "    'base':  'bert-base-uncased',\n",
    "    'large': 'bert-large-uncased',\n",
    "}\n",
    "\n",
    "\n",
    "model_name = size2name[model_size]\n",
    "\n",
    "force_whitespace_pre_tokeniser = False  # should not matter with pre-tokenised P&L'04 input\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "if force_whitespace_pre_tokeniser:\n",
    "    tokeniser.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rL-dSUZS1S_e",
   "metadata": {
    "id": "rL-dSUZS1S_e",
    "outputId": "6509cebd-c22e-4d21-af9a-b895c840d15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t['input_ids']: [101, 7592, 2088, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'hello', 'world', '!', '[SEP]']\n",
      "\t.word_ids():   [None, 0, 1, 2, None]\n",
      "             hello --> [7592]\n",
      "             world --> [2088]\n",
      "                 ! --> [999]\n",
      "1 \t['input_ids']: [101, 19204, 6648, 1005, 1055, 4569, 102]\n",
      "\ttokens:        ['[CLS]', 'token', '##isation', \"'\", 's', 'fun', '[SEP]']\n",
      "\t.word_ids():   [None, 0, 0, 1, 1, 2, None]\n",
      "      tokenisation --> [19204, 6648]\n",
      "                's --> [1005, 1055]\n",
      "               fun --> [4569]\n"
     ]
    }
   ],
   "source": [
    "# let's test the tokeniser with a small example\n",
    "\n",
    "example_batch = [\n",
    "    'hello world !'.split(),\n",
    "    \"\"\"tokenisation 's fun\"\"\".split(),\n",
    "]\n",
    "\n",
    "tokenised_text = tokeniser(\n",
    "    example_batch,  # pre-tokenised input\n",
    "    is_split_into_words = True,\n",
    ")\n",
    "\n",
    "for i, token_ids in enumerate(tokenised_text['input_ids']):\n",
    "    print(i, \"\\t['input_ids']:\", token_ids)\n",
    "    print(   '\\ttokens:       ', tokeniser.convert_ids_to_tokens(token_ids))\n",
    "    print(   \"\\t.word_ids():  \", tokenised_text.word_ids(batch_index = i))\n",
    "    example = example_batch[i]\n",
    "    for token in example:\n",
    "        # https://stackoverflow.com/questions/62317723/tokens-to-words-mapping-in-the-tokenizer-decode-step-huggingface\n",
    "        print('%18s --> %r' %(\n",
    "            token,\n",
    "            tokeniser.encode(\n",
    "                token,\n",
    "                add_special_tokens = False,\n",
    "            )\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cBHUA8DI1S_f",
   "metadata": {
    "id": "cBHUA8DI1S_f"
   },
   "source": [
    "## Sequence Length Distribution\n",
    "The BERT models made available to the public have been trained only up to a length of 512 subword units as memory requirements increase quadratically with the sequence length. We will restrict the sequence length further to 256 to keep resource requirements acceptable for mid-range GPUs. \n",
    "\n",
    "If many of our documents are longer than that it may pay off to look for a solution to using the full documents with BERT. For an overview of methods, see section 3.3.2 of\n",
    " * Palakh Mignonne Jude (2020). Increasing Accessibility of Electronic Theses and Dissertations (ETDs) Through Chapter-level Classification. PhD Thesis. Virginia Polytechnic Institute and State University, Blacksburg, Virginia, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kiXQsIX21S_f",
   "metadata": {
    "id": "kiXQsIX21S_f"
   },
   "outputs": [],
   "source": [
    "# if you have sufficient TPU or GPU memory you can increase the sequence length up to 512\n",
    "# (memory requirements will be the highest during fine-tuning of BERT)\n",
    "max_sequence_length  = 256\n",
    "\n",
    "# memory requirements increase linearly with the batch size;\n",
    "# a batch size of 16, better 32, is needed for efficient training;\n",
    "# however, with little memory, we have to go lower\n",
    "batch_size      = 10   # for a 12 GB card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jzKBoeom1S_f",
   "metadata": {
    "id": "jzKBoeom1S_f",
    "outputId": "a25e53eb-bc9d-40fb-b743-ae5be8c240d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LengthBin\tPos\tNeg\tTotal\n",
      "   0- 249\t7\t13\t20\n",
      " 250- 499\t126\t148\t274\n",
      " 500- 749\t271\t317\t588\n",
      " 750- 999\t275\t298\t573\n",
      "1000-1249\t171\t136\t307\n",
      "1250-1499\t75\t49\t124\n",
      "1500-1749\t37\t20\t57\n",
      "1750-1999\t21\t10\t31\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "bin_width = 250\n",
    "\n",
    "distribution = defaultdict(lambda: 0)\n",
    "\n",
    "# interate all reviews\n",
    "for fold in range(10):\n",
    "    for label in 'pos neg'.split():\n",
    "        batch = []\n",
    "        for document in data_loader.get_documents(\n",
    "            label = label,\n",
    "            fold = fold,\n",
    "        ):\n",
    "            tokens = []\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    tokens.append(token)\n",
    "            batch.append(tokens)\n",
    "        tokenised_batch = tokeniser(\n",
    "            batch,  # pre-tokenised input\n",
    "            is_split_into_words = True,\n",
    "        )\n",
    "        max_length_bin = 0\n",
    "        for token_ids in tokenised_batch['input_ids']:\n",
    "            length = len(token_ids)\n",
    "            length_bin = length // bin_width\n",
    "            distribution[(label,   length_bin)] += 1\n",
    "            distribution[('total', length_bin)] += 1\n",
    "            if length_bin > max_length_bin:\n",
    "                max_length_bin = length_bin\n",
    "print('LengthBin\\tPos\\tNeg\\tTotal')\n",
    "for length_bin in range(0, max_length_bin+1):\n",
    "    row = []\n",
    "    row.append('%4d-%4d' %(\n",
    "        bin_width*length_bin,\n",
    "        bin_width*(1+length_bin)-1\n",
    "    ))\n",
    "    for label in 'pos neg total'.split():\n",
    "        count = distribution[(label, length_bin)]\n",
    "        row.append('%d' %count)        \n",
    "    print('\\t'.join(row))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kE6YL-5j1S_g",
   "metadata": {
    "id": "kE6YL-5j1S_g"
   },
   "source": [
    "Conclusion: Truncating the documents to the sequence length of the model limits the amount of information available from each document. The following list some options for how we truncate the documents in order of complexity. \n",
    "1. Use the start of each document only, i.e. truncate the reviews. Using only the start of each review is also implemented in [Huggingface's example for fine-tuning on custom datasets](https://huggingface.co/transformers/custom_datasets.html).\n",
    "2. Use the end of each document only, i.e. take a slice from the end.\n",
    "3. Combine two slices of the document, one from the start and one from the end. What fraction of the sequence to use for the first slice is a parameter of this method. Setting the parameter to 0 or 1 yields methods 1 and 2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "poU0EVDR1S_g",
   "metadata": {
    "id": "poU0EVDR1S_g"
   },
   "source": [
    "## PyTorch DataLoader\n",
    "\n",
    "* Dataset objects behave like a list of dictionaries, one dictionary for each data instance (training or test item), with user-defined keys.\n",
    "* DataLoader objects shuffle data provided by a Dataset object and create batches of data.\n",
    "* LightningDataModule objects create DataLoader objects for training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-LYghZi01S_g",
   "metadata": {
    "id": "-LYghZi01S_g"
   },
   "outputs": [],
   "source": [
    "# basic usage of pytorch and lightning from\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# and\n",
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "class SlicedDocuments_part_1(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_data,\n",
    "        tokeniser = None,\n",
    "        fraction_for_first_sequence = 0.5,\n",
    "        max_sequence_length = 128,\n",
    "        second_part_as_sequence_B = False,\n",
    "        preproc_batch_size = 8,\n",
    "        \n",
    "    ):\n",
    "        '''Extracts slices from labelled documents\n",
    "           Args:\n",
    "               raw_data: list of (document, label) pairs\n",
    "               tokeniser: transformer tokeniser to obtain subword units; this tokeniser\n",
    "                   will be used to decide how many words to include in each slice; it\n",
    "                   is recommended to use the same tokeniser that will be used to tokenise\n",
    "                   the data\n",
    "               fraction_for_first_sequence: 1 = take slice from the start of the document,\n",
    "                   0 = take slice from the end of the document, > 0 and < 1 = take two\n",
    "                   slices, one of this relative size from the start and then fill to\n",
    "                   max_sequence_length from the end of the document\n",
    "               max_sequence_length: produce sequences up to this length\n",
    "               allow_partial_tokens: whether to slice between subword units of tokens; if\n",
    "                   set to True the produced sequences will always have max_sequence_length\n",
    "                   items unless the document is very short\n",
    "        '''\n",
    "        assert max_sequence_length >= 5\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.second_part_as_sequence_B = second_part_as_sequence_B\n",
    "        self.init_sequence_lengths(fraction_for_first_sequence)\n",
    "        self.tokeniser = tokeniser\n",
    "        self.init_slices(raw_data, preproc_batch_size)\n",
    "\n",
    "    def init_sequence_lengths(self, fraction_for_first_sequence):\n",
    "        available_for_two_sequences = self.max_sequence_length - 3\n",
    "        self.first_sequence_length = max(0, int(\n",
    "            fraction_for_first_sequence * available_for_two_sequences\n",
    "        ))\n",
    "        self.last_sequence_length = max(\n",
    "            0,\n",
    "            available_for_two_sequences - self.first_sequence_length\n",
    "        )\n",
    "        assert self.first_sequence_length \\\n",
    "            + self.last_sequence_length   \\\n",
    "            <= available_for_two_sequences\n",
    "        if self.first_sequence_length == 0:\n",
    "            self.last_sequence_length += 1\n",
    "        elif self.last_sequence_length == 0:\n",
    "            self.first_sequence_length += 1\n",
    "        if self.first_sequence_length == 0 \\\n",
    "        or self.last_sequence_length == 0:\n",
    "            # do not use a second [SEP] marker when there is\n",
    "            # always only one sequence\n",
    "            self.second_part_as_sequence_B = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kv7CTY0e1S_h",
   "metadata": {
    "id": "kv7CTY0e1S_h"
   },
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_2(SlicedDocuments_part_1):\n",
    "    \n",
    "    def init_slices(self, raw_data, preproc_batch_size):\n",
    "        self.slices = []\n",
    "        next_batch = []\n",
    "        next_labels = []\n",
    "        for document, label in raw_data:\n",
    "            tokens = []\n",
    "            for sentence in document:\n",
    "                tokens = tokens + sentence\n",
    "            next_batch.append(tokens)    \n",
    "            next_labels.append(label)\n",
    "            if len(next_batch) >= preproc_batch_size:\n",
    "                self.add_batch(next_batch, next_labels)\n",
    "                next_batch = []\n",
    "                next_labels = []\n",
    "        if next_batch:\n",
    "            self.add_batch(next_batch, next_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hAHAaEq51S_h",
   "metadata": {
    "id": "hAHAaEq51S_h"
   },
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_3(SlicedDocuments_part_2):\n",
    "    \n",
    "    def add_batch(self, batch, labels):\n",
    "        # determine, for each document in the batch, how many\n",
    "        # tokens to include from the start of the document\n",
    "        if self.first_sequence_length:\n",
    "            lengths_1 = self.get_lengths(batch)\n",
    "        else:\n",
    "            lengths_1 = len(batch) * [0]\n",
    "        # determine, for each document in the batch, how many\n",
    "        # tokens to include from the end of the document    \n",
    "        if self.last_sequence_length:\n",
    "            lengths_2 = self.get_lengths(batch, part = 2, lengths_1 = lengths_1)\n",
    "        else:\n",
    "            lengths_2 = len(batch) * [0]\n",
    "        # TODO: In an earlier version, we did not check the following\n",
    "        #       condition, creating a second sequence for short\n",
    "        #       documents even though only one sequence is requested.\n",
    "        #       First results indicate that this bug actually\n",
    "        #       improves performance. Future work should investigate\n",
    "        #       this and, if this effect is confirmed, propose a\n",
    "        #       clean solution to exploit this effect.\n",
    "        if self.first_sequence_length:\n",
    "            # sometimes there is space for more tokens from the\n",
    "            # start even though no more from the end fit\n",
    "            lengths_1 = self.expand_lengths(batch, lengths_1, lengths_2)\n",
    "        # TODO: For cases with length_1 + length_2 > len(tokens),\n",
    "        #       should we adjust the parts to not overlap?       \n",
    "        # prepare texts\n",
    "        for batch_idx, tokens in enumerate(batch):\n",
    "            parts = []\n",
    "            length_1 = lengths_1[batch_idx]\n",
    "            length_2 = lengths_2[batch_idx]\n",
    "            part_1 = tokens[:length_1]\n",
    "            if length_2 > 0:\n",
    "                part_2 = tokens[-length_2:]\n",
    "            else:\n",
    "                part_2 = []\n",
    "            if self.second_part_as_sequence_B:\n",
    "                parts.append(part_1)\n",
    "                parts.append(part_2)\n",
    "            else:\n",
    "                parts.append(part_1 + part_2)\n",
    "            assert len(parts) > 0    \n",
    "            self.slices.append((parts, labels[batch_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IqrJRTKj1S_i",
   "metadata": {
    "id": "IqrJRTKj1S_i"
   },
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_4(SlicedDocuments_part_3):\n",
    "\n",
    "    def get_lengths(\n",
    "        self, batch, part = 1, lengths_1 = None,\n",
    "        lengths_2 = None,\n",
    "    ):\n",
    "        if part == 3:\n",
    "            lower_limits = lengths_1[:]  # clone\n",
    "        else:\n",
    "            lower_limits = len(batch) * [0]\n",
    "        upper_limits = []\n",
    "        for tokens in batch:\n",
    "            upper_limits.append(min(\n",
    "                len(tokens),\n",
    "                self.max_sequence_length\n",
    "            ))\n",
    "        # we want each upper limit to be a sequence length\n",
    "        # that is too big but sometimes the full document\n",
    "        # (or max_length words) can fit --> test for this\n",
    "        # special case\n",
    "        for batch_idx, fit in enumerate(\n",
    "            self.get_fit(batch, upper_limits, part, lengths_1, lengths_2)\n",
    "        ):\n",
    "            if fit:\n",
    "                # update lower limit to match upper limit\n",
    "                # to mark this document as not needing any\n",
    "                # further length search\n",
    "                lower_limits[batch_idx] = upper_limits[batch_idx]\n",
    "        while True:\n",
    "            # prepare next lengths to test and check whether search is finished\n",
    "            new_limits = []\n",
    "            all_done = True\n",
    "            for batch_idx, tokens in enumerate(batch):\n",
    "                if lower_limits[batch_idx]+1 >= upper_limits[batch_idx]:\n",
    "                    new_limits.append(lower_limits[batch_idx])\n",
    "                else:\n",
    "                    all_done = False\n",
    "                    new_limits.append((\n",
    "                        lower_limits[batch_idx] + upper_limits[batch_idx]\n",
    "                    )//2)\n",
    "            if all_done:\n",
    "                return lower_limits\n",
    "            # adjust lower and upper limits\n",
    "            for batch_idx, fit in enumerate(\n",
    "                self.get_fit(batch, new_limits, part, lengths_1, lengths_2)\n",
    "            ):\n",
    "                if fit:\n",
    "                    lower_limits[batch_idx] = new_limits[batch_idx]\n",
    "                else:\n",
    "                    upper_limits[batch_idx] = new_limits[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T_Ue1NWQ1S_j",
   "metadata": {
    "id": "T_Ue1NWQ1S_j"
   },
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_5(SlicedDocuments_part_4):\n",
    "\n",
    "    def get_fit(self, batch, limits, part, lengths_1, lengths_2 = None):\n",
    "        sliced_batch_A = []\n",
    "        sliced_batch_B = []\n",
    "        for batch_idx, tokens in enumerate(batch):\n",
    "            if part == 1:\n",
    "                length_1 = limits[batch_idx]\n",
    "                length_2 = 0\n",
    "            elif part == 2:\n",
    "                length_1 = lengths_1[batch_idx]\n",
    "                length_2 = limits[batch_idx]\n",
    "            else:\n",
    "                length_1 = limits[batch_idx]\n",
    "                length_2 = lengths_2[batch_idx]\n",
    "            part_1 = tokens[:length_1]\n",
    "            if length_2 > 0:\n",
    "                part_2 = tokens[-length_2:]\n",
    "            else:\n",
    "                part_2 = []\n",
    "            if self.second_part_as_sequence_B:\n",
    "                sliced_batch_A.append(part_1)\n",
    "                sliced_batch_B.append(part_2)\n",
    "            else:\n",
    "                sliced_batch_A.append(part_1 + part_2)\n",
    "        if self.second_part_as_sequence_B:\n",
    "            tokenised = self.tokeniser(\n",
    "               sliced_batch_A, sliced_batch_B,\n",
    "               is_split_into_words = True,\n",
    "            )\n",
    "        else:\n",
    "            tokenised = self.tokeniser(\n",
    "               sliced_batch_A,\n",
    "               is_split_into_words = True,\n",
    "            )\n",
    "        # check lengths in subword pieced\n",
    "        retval = []\n",
    "        for batch_idx, subword_ids in enumerate(tokenised['input_ids']):\n",
    "            if part == 1:\n",
    "                length = len(subword_ids) - 2  # account for [CLS] and [SEP]\n",
    "                if self.second_part_as_sequence_B:\n",
    "                    length -= 1                # account for second [SEP] token\n",
    "                fit = length <= self.first_sequence_length\n",
    "            else:\n",
    "                fit = len(subword_ids) <= self.max_sequence_length\n",
    "            retval.append(fit)\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QyFBaVpv1S_l",
   "metadata": {
    "id": "QyFBaVpv1S_l"
   },
   "outputs": [],
   "source": [
    "class SlicedDocuments(SlicedDocuments_part_5):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            assert isinstance(idx, int)\n",
    "        parts, label = self.slices[idx]\n",
    "        retval = {}\n",
    "        retval['parts'] = parts\n",
    "        retval['label'] = label\n",
    "        return retval\n",
    "    \n",
    "    def expand_lengths(self, batch, lengths_1, lengths_2):\n",
    "        ''' pushes lengths_1 as far out as possible '''\n",
    "        return self.get_lengths(\n",
    "            batch, part = 3,\n",
    "            lengths_1 = lengths_1,\n",
    "            lengths_2 = lengths_2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5HvMC7GY1S_l",
   "metadata": {
    "id": "5HvMC7GY1S_l"
   },
   "source": [
    "## Create Training-Test Splits for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BuCRUzNu1S_l",
   "metadata": {
    "id": "BuCRUzNu1S_l",
    "outputId": "10d706f0-b998-4dae-d9ed-73b397ce0984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "splits = data_loader.get_xval_splits()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0rCG_581S_l",
   "metadata": {
    "id": "f0rCG_581S_l"
   },
   "source": [
    "## Test Above Dataset Class\n",
    "Test Slicing of Documents to BERT Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41g-TtmV1S_m",
   "metadata": {
    "id": "41g-TtmV1S_m",
    "outputId": "4e0f228f-2b56-4174-8221-bba2fbd67362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 training documents ready in x-val fold 0\n"
     ]
    }
   ],
   "source": [
    "# this may take a minute\n",
    "\n",
    "raw_data = splits[0][0]\n",
    "sliced_docs = SlicedDocuments(\n",
    "    raw_data, tokeniser,\n",
    "    fraction_for_first_sequence = 0.25,\n",
    "    max_sequence_length = max_sequence_length, # 99,\n",
    "    second_part_as_sequence_B = True,\n",
    ")\n",
    "print('%d training documents ready in x-val fold 0' %len(sliced_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MqQDmg3j1S_m",
   "metadata": {
    "id": "MqQDmg3j1S_m",
    "outputId": "28aed5d0-df3b-4741-ca91-cc122a321019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_idx seq_len tokens1 tokens2 total\n",
      "      0     256      59     175   234\n",
      "      1     256      59     159   218\n",
      "      2     256      50     167   217\n",
      "      3     256      53     178   231\n",
      "      4     256      46     177   223\n",
      "      5     256      50     150   200\n",
      "      6     256      61     148   209\n",
      "      7     256      58     171   229\n",
      "      8     256      51     174   225\n",
      "      9     256      48     146   194\n",
      "\n",
      "First and last 3 sentences of doc_idx 906 with 53 seq_len:\n",
      "[0] ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "Selected slice 1: ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "Selected slice 2: ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "\n",
      "Frequency of each sequence length:\n",
      "53 1\n",
      "243 1\n",
      "254 5\n",
      "255 35\n",
      "256 1758\n"
     ]
    }
   ],
   "source": [
    "length2count = defaultdict(lambda: 0)\n",
    "\n",
    "print('doc_idx seq_len tokens1 tokens2 total')\n",
    "for doc_idx, sliced_doc in enumerate(sliced_docs):\n",
    "    parts = sliced_doc['parts']\n",
    "    if len(parts) == 2:\n",
    "        tokenised = tokeniser(\n",
    "           [parts[0]],\n",
    "           [parts[1]],\n",
    "           is_split_into_words = True,\n",
    "        )\n",
    "        tokens1 = len(parts[0])\n",
    "        tokens2 = len(parts[1])\n",
    "    else:\n",
    "        tokenised = tokeniser(\n",
    "           [parts[0]],\n",
    "           is_split_into_words = True,\n",
    "        )\n",
    "        tokens1 = len(parts[0])\n",
    "        tokens2 = 0\n",
    "    length = len(tokenised['input_ids'][0])\n",
    "    length2count[length] += 1             \n",
    "\n",
    "    if length < 90:\n",
    "        print(\n",
    "            '\\nFirst and last 3 sentences of doc_idx', doc_idx,\n",
    "            'with', length, 'seq_len:'\n",
    "        )\n",
    "        sent_idx = set()\n",
    "        for idx in range(3):\n",
    "            sent_idx.add(idx)\n",
    "            idx = len(raw_data[doc_idx][0]) - 1 - idx\n",
    "            if idx >= 0:\n",
    "                sent_idx.add(idx)\n",
    "        for s_idx in sorted(list(sent_idx)):\n",
    "            try:\n",
    "                sentence = raw_data[doc_idx][0][s_idx]\n",
    "                print('[%d] %r' %(s_idx, sentence))\n",
    "            except IndexError:\n",
    "                pass\n",
    "        print('Selected slice 1:', parts[0])\n",
    "        if len(parts) == 2:\n",
    "            print('Selected slice 2:', parts[1])\n",
    "    if doc_idx < 10:\n",
    "        print('%7d %7d %7d %7d %5d' %(\n",
    "            doc_idx, length, tokens1, tokens2, tokens1+tokens2,\n",
    "        ))\n",
    "        \n",
    "print('\\nFrequency of each sequence length:')        \n",
    "for length in sorted(list(length2count.keys())):\n",
    "    print(length, length2count[length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XoEqDwAl1S_m",
   "metadata": {
    "id": "XoEqDwAl1S_m"
   },
   "outputs": [],
   "source": [
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "class SlicedDataModule_Part_1(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, classifier, data_split = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hparams = classifier.hparams\n",
    "        self.classifier = classifier\n",
    "        if data_split is None:\n",
    "            # this happens when loading a checkpoint\n",
    "            data_split = (None, None, None)\n",
    "        elif len(data_split) == 2:\n",
    "            # add empty validation set\n",
    "            tr_data, val_data = self.split(data_split[0], 0.9)\n",
    "            data_split = (tr_data, val_data, data_split[1])\n",
    "        self.data_split = data_split\n",
    "        self.kwargs = kwargs\n",
    "        self.label_encoder = LabelEncoder(\n",
    "            ['pos', 'neg'],\n",
    "            reserved_labels = [],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        dataset = SlicedDocuments(\n",
    "            raw_data = self.data_split[0],\n",
    "            **self.kwargs\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset = dataset,\n",
    "            sampler     = RandomSampler(dataset),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lBDmAsl11S_n",
   "metadata": {
    "id": "lBDmAsl11S_n"
   },
   "outputs": [],
   "source": [
    "class SlicedDataModule(SlicedDataModule_Part_1):\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        if not self.data_split[1]:\n",
    "            return None   # TODO: check documentation what to return\n",
    "        return DataLoader(\n",
    "            dataset = SlicedDocuments(\n",
    "                raw_data = self.data_split[1],\n",
    "                **self.kwargs\n",
    "            ),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        return DataLoader(\n",
    "            dataset = SlicedDocuments(\n",
    "                raw_data = self.data_split[2],\n",
    "                **self.kwargs\n",
    "            ),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def split(self, data, ratio):\n",
    "        # get label distribution:\n",
    "        distribution = defaultdict(lambda: 0)\n",
    "        for _, label in data:\n",
    "            distribution[label] += 1\n",
    "        # get target frequencies of labels in first set\n",
    "        still_needed = defaultdict(lambda: 0)\n",
    "        for label in distribution:\n",
    "            still_needed[label] = int(ratio*distribution[label])\n",
    "        # split data accordingly\n",
    "        dataset_1 = []\n",
    "        dataset_2 = []\n",
    "        for item in data:\n",
    "            label = item[1]\n",
    "            if still_needed[label] > 0:\n",
    "                dataset_1.append(item)\n",
    "                still_needed[label] -= 1\n",
    "            else:\n",
    "                dataset_2.append(item)\n",
    "        return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bosRYZUa1S_n",
   "metadata": {
    "id": "bosRYZUa1S_n"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_part_1(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict:\n",
    "            #print('Converting', type(hparams))\n",
    "            hparams = pl.utilities.AttributeDict(hparams)\n",
    "        #print('New classifier with', hparams)\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.data = SlicedDataModule(self, **kwargs)\n",
    "        if 'tokeniser' in kwargs:\n",
    "            self.tokenizer = kwargs['tokeniser']  # attribute expected by lightning\n",
    "        else:\n",
    "            # this happens when loading a checkpoint\n",
    "            self.tokenizer = None  # TODO: this may break ability to use the model\n",
    "        self.__build_model()\n",
    "        self.__build_loss()\n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "        self.record_predictions = False\n",
    "            \n",
    "    def __build_model(self) -> None:\n",
    "        ''' Init BERT model, tokeniser and classification head '''\n",
    "        # Q: Why not use AutoModelForSequenceClassification?\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name,  # was: self.hparams.encoder_model\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1536),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1536, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, self.data.label_encoder.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __build_loss(self):\n",
    "        self._loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zfca-vI01S_n",
   "metadata": {
    "id": "Zfca-vI01S_n"
   },
   "outputs": [],
   "source": [
    "import logging as log\n",
    "\n",
    "class Classifier_part_2(Classifier_part_1):\n",
    "    \n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        if self._frozen:\n",
    "            log.info('\\n== Encoder model fine-tuning ==')\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "            \n",
    "    def freeze_encoder(self) -> None:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_inputs, _ = self.prepare_sample(\n",
    "                [sample],\n",
    "                prepare_target = False\n",
    "            )\n",
    "            model_out = self.forward(batch_inputs)\n",
    "            logits = torch.Tensor.cpu(model_out[\"logits\"]).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits, axis=1)\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "        return sample\n",
    "    \n",
    "    def start_recording_predictions(self):\n",
    "        self.record_predictions = True\n",
    "        self.reset_recorded_predictions()\n",
    "        \n",
    "    def stop_recording_predictions(self):\n",
    "        self.record_predictions = False\n",
    "        \n",
    "    def reset_recorded_predictions(self):\n",
    "        self.seq2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3srjxU4B1S_o",
   "metadata": {
    "id": "3srjxU4B1S_o"
   },
   "outputs": [],
   "source": [
    "from torchnlp.utils import lengths_to_mask\n",
    "\n",
    "class Classifier_part_3(Classifier_part_2):\n",
    "    \n",
    "    def forward(self, batch_input):\n",
    "        tokens  = batch_input['input_ids']\n",
    "        lengths = batch_input['length']\n",
    "        mask = batch_input['attention_mask']\n",
    "        # Run BERT model.\n",
    "        word_embeddings = self.bert(tokens, mask).last_hidden_state\n",
    "        sentemb = word_embeddings[:,0]  # at position of [CLS]\n",
    "        logits = self.classification_head(sentemb)\n",
    "        # Hack to conveniently use the model and trainer to\n",
    "        # get predictions for a test set:\n",
    "        if self.record_predictions:\n",
    "            logits_np = torch.Tensor.cpu(logits).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits_np, axis=1)\n",
    "            ]\n",
    "            for index, input_token_ids in enumerate(tokens):\n",
    "                key = torch.Tensor.cpu(input_token_ids).numpy().tolist()\n",
    "                # truncate trailing zeros\n",
    "                while key and key[-1] == 0:\n",
    "                    del key[-1]\n",
    "                self.seq2label[tuple(key)] = predicted_labels[index]\n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "    def loss(self, predictions: dict, targets: dict) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: model specific output. Must contain a key 'logits' with\n",
    "            a tensor [batch_size x 1] with model predictions\n",
    "        :param labels: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions[\"logits\"], targets[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3gt-s-h1S_o",
   "metadata": {
    "id": "p3gt-s-h1S_o"
   },
   "outputs": [],
   "source": [
    "class Classifier_part_4(Classifier_part_3):\n",
    "    \n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Function that prepares a sample to input the model.\n",
    "        :param sample: list of dictionaries.\n",
    "        \n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        assert len(sample) <= batch_size\n",
    "        assert self.tokenizer is not None\n",
    "        with_1_part = 0\n",
    "        with_2_parts = 0\n",
    "        batch_part_1 = []\n",
    "        batch_part_2 = []\n",
    "        for item in sample:\n",
    "            parts = item['parts']\n",
    "            if len(parts) == 2:\n",
    "                with_2_parts += 1\n",
    "                batch_part_1.append(parts[0])\n",
    "                batch_part_2.append(parts[1])\n",
    "            else:\n",
    "                with_1_part += 1\n",
    "                batch_part_1.append(parts[0])\n",
    "        assert not (with_1_part and with_2_parts)\n",
    "        kwargs = {\n",
    "            'is_split_into_words': True,\n",
    "            'return_length':       True,\n",
    "            'padding':             'max_length',\n",
    "            # https://github.com/huggingface/transformers/issues/8691\n",
    "            'return_tensors':      'pt',\n",
    "        }\n",
    "        if with_2_parts:\n",
    "            encoded_batch = self.tokenizer(\n",
    "                batch_part_1,\n",
    "                batch_part_2,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            encoded_batch = self.tokenizer(\n",
    "                batch_part_1,\n",
    "                **kwargs\n",
    "            )\n",
    "        if not prepare_target:\n",
    "            return encoded_batch, {}\n",
    "        # Prepare target:\n",
    "        batch_labels = []\n",
    "        for item in sample:\n",
    "            batch_labels.append(item['label'])\n",
    "        assert len(batch_labels) <= batch_size\n",
    "        try:\n",
    "            targets = {\n",
    "                \"labels\": self.data.label_encoder.batch_encode(batch_labels)\n",
    "            }\n",
    "            return encoded_batch, targets\n",
    "        except RuntimeError:\n",
    "            raise Exception(\"Label encoder found an unknown label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xsWg4D201S_o",
   "metadata": {
    "id": "xsWg4D201S_o"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Classifier_part_5(Classifier_part_4):\n",
    "    \n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        # Q: What is this about?\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        output = OrderedDict({\"loss\": loss_val})\n",
    "        self.log('train_loss', loss_val, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "   \n",
    "    def test_or_validation_step(self, test_type, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        y = targets[\"labels\"]\n",
    "        y_hat = model_out[\"logits\"]\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "        output = OrderedDict({\n",
    "            test_type + \"_loss\": loss_val,\n",
    "            test_type + \"_acc\":  val_acc,\n",
    "            'batch_size': len(batch),\n",
    "            #'predictions': labels_hat,\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'val', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "    \n",
    "    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'test', batch, batch_nb, *args, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tDsySDvr1S_p",
   "metadata": {
    "id": "tDsySDvr1S_p"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class Classifier(Classifier_part_5):\n",
    "    \n",
    "    # validation_end() is now validation_epoch_end()\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/blob/efd272a3cac2c412dd4a7aa138feafb2c114326f/CHANGELOG.md\n",
    "    \n",
    "    def test_or_validation_epoch_end(self, test_type, outputs: list) -> None:\n",
    "        val_loss_mean = 0.0\n",
    "        val_acc_mean = 0.0\n",
    "        total_size = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output[test_type + \"_loss\"]\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output[test_type + \"_acc\"]\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "            # We weight the batch accuracy by batch size to not give\n",
    "            # higher weight to the items of a smaller, final bacth.\n",
    "            batch_size = output['batch_size']\n",
    "            val_acc_mean += val_acc * batch_size\n",
    "            total_size += batch_size\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= total_size\n",
    "        self.log(test_type+'_loss', val_loss_mean)\n",
    "        self.log(test_type+'_acc',  val_acc_mean)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('val', outputs)\n",
    "                                     \n",
    "    def test_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('test', outputs)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.classification_head.parameters()},\n",
    "            {\n",
    "                \"params\": self.bert.parameters(),\n",
    "                \"lr\": self.hparams.encoder_learning_rate,\n",
    "                #\"weight_decay\": 0.01,  # TODO: try this as it is in the BERT paper\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fk1pXMdb1S_p",
   "metadata": {
    "id": "Fk1pXMdb1S_p",
    "outputId": "29034c5d-ba4e-4efc-eb8c-9e30b53c2b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 10\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "xval_run = 0  # 0 to 9\n",
    "\n",
    "\n",
    "#class DotDict(pl.utilities.AttributeDict):    \n",
    "#    __getattr__ = dict.get\n",
    "\n",
    "    # the above misses pickle support; presumable dict defines custom pickle behaviour\n",
    "    # that causes the unpickled object to be a plain dict object\n",
    "\n",
    "print('batch_size', batch_size)\n",
    "\n",
    "classifier = Classifier(\n",
    "    hparams = {\n",
    "        \"encoder_learning_rate\": 1e-05,  # Encoder specific learning rate\n",
    "        \"learning_rate\":         3e-05,  # Classification head learning rate\n",
    "        \"nr_frozen_epochs\":      3,      # Number of epochs we want to keep the encoder model frozen\n",
    "        \"loader_workers\":        4,      # How many subprocesses to use for data loading.\n",
    "                                         # (0 means that the data will be loaded in the main process)\n",
    "        \"batch_size\":            batch_size,\n",
    "        \"gpus\":                  1,\n",
    "    },\n",
    "    # parameters for SlicedDataModule:\n",
    "    data_split = splits[xval_run],\n",
    "    # parameters for SlicedDocument():\n",
    "    tokeniser                   = tokeniser,\n",
    "    fraction_for_first_sequence = 0.0,   # set to 0.0001 to duplicate short documents\n",
    "    max_sequence_length         = max_sequence_length,\n",
    "    second_part_as_sequence_B   = False,\n",
    "    preproc_batch_size          = 8\n",
    ")   \n",
    "print('Ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dy_jZy7C1S_p",
   "metadata": {
    "id": "Dy_jZy7C1S_p",
    "outputId": "6846ae03-89fc-4b75-f439-aa99936b4b71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | bert                | BertModel        | 109 M \n",
      "1 | classification_head | Sequential       | 1.6 M \n",
      "2 | _loss               | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "111 M     Total params\n",
      "444.230   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf2803362064f65b5b5959f4bc7d80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 21 minutes\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor   = 'val_acc',\n",
    "    min_delta = 0.00,\n",
    "    patience  = 5,\n",
    "    verbose   = False,\n",
    "    mode      = 'max',\n",
    ")\n",
    "\n",
    "save_top_model_callback = ModelCheckpoint(\n",
    "    save_top_k = 3,\n",
    "    monitor    = 'val_acc',\n",
    "    mode       = 'max',\n",
    "    filename   = '{val_acc:.4f}-{epoch:02d}-{val_loss:.4f}'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stop_callback, save_top_model_callback],\n",
    "    max_epochs = 6,\n",
    "    min_epochs = classifier.hparams.nr_frozen_epochs + 2,\n",
    "    gpus = classifier.hparams.gpus,\n",
    "    accumulate_grad_batches = 4,   # compensate for small batch size\n",
    "    #limit_train_batches = 10,  # use only a subset of the data during development for higher speed\n",
    "    check_val_every_n_epoch = 1,\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "start = time.time()\n",
    "trainer.fit(classifier, classifier.data)\n",
    "print('Training time: %.0f minutes' %((time.time()-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QfW4E2UI1S_q",
   "metadata": {
    "id": "QfW4E2UI1S_q",
    "outputId": "39c82309-7f94-4edc-81ea-aeabec9a537d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7c7917e570a868f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7c7917e570a868f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=nCq_vy9qE-k at 44:59\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OM5PvIXk1S_q",
   "metadata": {
    "id": "OM5PvIXk1S_q",
    "outputId": "858f4520-5da5-4695-9113-edc044f1e804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is /home/jwagner/bert/teaching-CA4023-NLP/CA4023-NLP/notebooks/lightning_logs/default/version_28/checkpoints/val_acc=0.9500-epoch=05-val_loss=0.1351.ckpt\n",
      "Best validation set accuracy: tensor(0.9500, device='cuda:0')\n",
      "Test results via trainer.test():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc9925a82d74473a483643b7fb3c100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8949999213218689, 'test_loss': 0.21454036235809326}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('The best model is', save_top_model_callback.best_model_path)\n",
    "\n",
    "print('Best validation set accuracy:', save_top_model_callback.best_model_score)\n",
    "\n",
    "# The following automatically loads the best weights according to\n",
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "\n",
    "print('Test results via trainer.test():')\n",
    "results = trainer.test()  # also prints results as a side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27R-BOeB1S_q",
   "metadata": {
    "id": "27R-BOeB1S_q"
   },
   "source": [
    "## Save Best Model outside Logs\n",
    "\n",
    "Rather than manually locating the best model in the lightning logs folder and copying it to another location, use the  library to save a copy. This also gives us the option to save a copy without the training state of the Adam optimiser, reducing model size by about 67%, training parameters and filesystem paths that we may not want to share with users of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B3wnEclM1S_r",
   "metadata": {
    "id": "B3wnEclM1S_r",
    "outputId": "f3c14329-1a5c-4eb0-db3f-aaf526520768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html\n",
    "\n",
    "# after just having run test(), the best checkpoint is still loaded but that's\n",
    "# not a documented feature so to be on the safe side for future versions we\n",
    "# need to explicitly load the best checkpoint:\n",
    "\n",
    "best_model = Classifier.load_from_checkpoint(\n",
    "    checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "    # the hparams including hparams.batch_size appear to have been\n",
    "    # saved in the checkpoint automatically\n",
    ")\n",
    "# best_model.save_checkpoint('best.ckpt') does not exist\n",
    "# --> need to wrap model into trainer to be able to save a checkpoint\n",
    "\n",
    "new_trainer = pl.Trainer(\n",
    "    resume_from_checkpoint = trainer.checkpoint_callback.best_model_path,\n",
    "    gpus = -1,  # avoid warnings (-1 = automatic selection)\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "new_trainer.model = best_model  # @model.setter in plugins/training_type/training_type_plugin.py\n",
    "\n",
    "#new_trainer.save_checkpoint(\"best-model.ckpt\")  # contains absoulte paths and training parameters\n",
    "\n",
    "new_trainer.save_checkpoint(\n",
    "    \"best-model-weights-only.ckpt\",\n",
    "    True,  # save_weights_only\n",
    ")\n",
    "\n",
    "# to just save the bert model in pytorch format and without the classification head, we could follow\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3096#issuecomment-686877242\n",
    "best_model.bert.save_pretrained('best-bert-encoder.pt')\n",
    "\n",
    "# Since the lightning module inherits from pytorch, we can save the full network in\n",
    "# pytorch format:\n",
    "torch.save(best_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I0S7QyhE1S_s",
   "metadata": {
    "id": "I0S7QyhE1S_s"
   },
   "source": [
    "Note: The `.ckpt` files are zip files containing a [pickle](https://docs.python.org/3/library/pickle.html) file, version information and various binary files, presumably numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TttCZoE-1S_s",
   "metadata": {
    "id": "TttCZoE-1S_s"
   },
   "source": [
    "## Load a Model and Test Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2mkPK1Qm1S_s",
   "metadata": {
    "id": "2mkPK1Qm1S_s",
    "outputId": "4aeca248-2a6f-43f7-b5a5-67a675a104ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 20\n",
      "setting tokeniser\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a495e1c9bcb4bef801ed0359fe14944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8949999213218689, 'test_loss': 0.21454036235809326}\n",
      "--------------------------------------------------------------------------------\n",
      "[{'test_loss': 0.21454036235809326, 'test_acc': 0.8949999213218689}]\n"
     ]
    }
   ],
   "source": [
    "best_model = Classifier.load_from_checkpoint(\n",
    "    checkpoint_path = 'best-model-weights-only.ckpt'\n",
    ")\n",
    "\n",
    "best_model.eval()  # enter prediction mode, e.g. turn off dropout\n",
    "\n",
    "print(best_model.data.data_split)  # confirm the data is not saved\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = SlicedDocuments(\n",
    "        raw_data                    = splits[xval_run][-1],  # test data\n",
    "        tokeniser                   = tokeniser,\n",
    "        fraction_for_first_sequence = 0.0,\n",
    "        max_sequence_length         = max_sequence_length,\n",
    "        second_part_as_sequence_B   = False,\n",
    "        preproc_batch_size          = 8\n",
    "    ),\n",
    "    batch_size  = best_model.hparams.batch_size,\n",
    "    collate_fn  = best_model.prepare_sample,\n",
    "    num_workers = best_model.hparams.loader_workers,\n",
    ")\n",
    "\n",
    "print('number of batches:', len(test_dataloader))\n",
    "\n",
    "new_trainer = pl.Trainer(\n",
    "    gpus = -1,\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "\n",
    "if best_model.tokenizer is None:\n",
    "    print('setting tokeniser')\n",
    "    best_model.tokenizer = tokeniser\n",
    "\n",
    "print(new_trainer.test(best_model, test_dataloaders = [test_dataloader]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZAf76Xm1S_s",
   "metadata": {
    "id": "sZAf76Xm1S_s"
   },
   "source": [
    "## Make Predictions\n",
    "\n",
    "Pytorch_lightning does not seem to provide functionality to re-use above code for making predictions. The example code from their website directly calls the `forward()` function of the model, assuming that the inputs of the test items are ready in a suitable batch. For a small test set that does not exceed the batch size, we can manally create such as a batch as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BywMwdo81S_t",
   "metadata": {
    "id": "BywMwdo81S_t"
   },
   "source": [
    "### Small Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KgxLqKSN1S_t",
   "metadata": {
    "id": "KgxLqKSN1S_t",
    "outputId": "7ab1929f-9279-45bb-d0b7-8c46f2459f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cuda:0\n",
      "number of documents: 2\n"
     ]
    }
   ],
   "source": [
    "# reminder: In our dataset, documents are lists of sentences\n",
    "# and each sentence is a list of words and punctuation\n",
    "\n",
    "mini_test_set  = [\n",
    "    # document 1\n",
    "    ([\n",
    "        'This movie is great .'.split(),\n",
    "        'So much fun .'.split(),\n",
    "    ], 'pos'\n",
    "    ),\n",
    "    # document 2\n",
    "    ([\n",
    "        'What a waste of time .'.split(),\n",
    "        'Never seen anything this bad .'.split(),\n",
    "    ], 'neg'\n",
    "    ), \n",
    "]\n",
    "dataset = SlicedDocuments(  # subclass of torch.utils.data.Dataset\n",
    "    mini_test_set,\n",
    "    preproc_batch_size = 8,\n",
    "    # the following should match the trained model\n",
    "    tokeniser = tokeniser,\n",
    "    fraction_for_first_sequence = 0.0,  \n",
    "    max_sequence_length = max_sequence_length,\n",
    "    second_part_as_sequence_B = False,\n",
    ")\n",
    "\n",
    "print('model device:', best_model.device)\n",
    "\n",
    "print('number of documents:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxSg07yx1S_t",
   "metadata": {
    "id": "sxSg07yx1S_t",
    "outputId": "7b973236-6eae-4192-d1ae-1911f98690f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items in batch: 4\n",
      "number of predictions: 2\n",
      "[0] {'parts': [['This', 'movie', 'is', 'great', '.', 'So', 'much', 'fun', '.']], 'label': 'pos'}\n",
      "prediction: pos\n",
      "[1] {'parts': [['What', 'a', 'waste', 'of', 'time', '.', 'Never', 'seen', 'anything', 'this', 'bad', '.']], 'label': 'neg'}\n",
      "prediction: neg\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "encoded_batch, gold_labels = best_model.prepare_sample(dataset)\n",
    "print('number of items in batch:', len(encoded_batch))  # TODO: Why is this not len(dataset)?\n",
    "\n",
    "best_model.eval()  # just in case (already called further above)\n",
    "\n",
    "best_model.freeze()  # some examples call this before making predictions\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/5111\n",
    "encoded_batch.to(best_model.device)\n",
    "\n",
    "model_out = best_model(encoded_batch)\n",
    "\n",
    "# adjsuted copy of code from predict()\n",
    "logits = model_out[\"logits\"]\n",
    "logits = torch.Tensor.cpu(logits).numpy()\n",
    "predicted_labels = [\n",
    "    best_model.data.label_encoder.index_to_token[prediction]\n",
    "    for prediction in numpy.argmax(logits, axis=1)\n",
    "]\n",
    "\n",
    "print('number of predictions:', len(predicted_labels))  # matches len(dataset)\n",
    "\n",
    "for index, item in enumerate(dataset):\n",
    "    print('[%d]' %index, item)\n",
    "    print('prediction:', predicted_labels[index])\n",
    "    \n",
    "# the 'parts' list has two parts when second_part_as_sequence_B = True and fraction_for_first_sequence > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "np85gTr-1S_u",
   "metadata": {
    "id": "np85gTr-1S_u"
   },
   "source": [
    "### Large Test Sets\n",
    "For test sets that do not fit into a single batch, we extend the model's evaluation function to also record predictions in the metrics dictionary. We keep a record of the inputs as well as the test items may be distributed over multiple GPUs and the order of items may therefore change. We then only need to tokenise the test items again and fetch the predictions from the metrics dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVMWsARj1S_u",
   "metadata": {
    "id": "IVMWsARj1S_u",
    "outputId": "512a7b9d-25cc-400e-d301-1863fe2d224c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb97fe2429441f8b9edf794c8c44d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 1.0, 'test_loss': 0.01612304151058197}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# uses best_model, dataset and new_trainer from above\n",
    "\n",
    "best_model.start_recording_predictions()\n",
    "\n",
    "new_trainer.test(best_model, test_dataloaders = [DataLoader(\n",
    "    dataset     = dataset,   # let's first test the functionality with a small test set\n",
    "    batch_size  = best_model.hparams.batch_size,\n",
    "    collate_fn  = best_model.prepare_sample,\n",
    "    num_workers = best_model.hparams.loader_workers,\n",
    ")])\n",
    "\n",
    "best_model.stop_recording_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eJj3xii91S_u",
   "metadata": {
    "id": "eJj3xii91S_u",
    "outputId": "1e534602-6dac-4178-e320-4d0efe9f54d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(101, 2023, 3185, 2003, 2307, 1012, 2061, 2172, 4569, 1012, 102): 'pos', (101, 2054, 1037, 5949, 1997, 2051, 1012, 2196, 2464, 2505, 2023, 2919, 1012, 102): 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(best_model.seq2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xO4A6zmh1S_u",
   "metadata": {
    "id": "xO4A6zmh1S_u",
    "outputId": "1cfb2661-74b5-4a93-8f7d-bfc415281a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] {'parts': [['This', 'movie', 'is', 'great', '.', 'So', 'much', 'fun', '.']], 'label': 'pos'}\n",
      "prediction: pos\n",
      "[1] {'parts': [['What', 'a', 'waste', 'of', 'time', '.', 'Never', 'seen', 'anything', 'this', 'bad', '.']], 'label': 'neg'}\n",
      "prediction: neg\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(dataset):\n",
    "    print('[%d]' %index, item)\n",
    "    input_token_ids = best_model.prepare_sample([item])[0]['input_ids']\n",
    "    key = input_token_ids.tolist()[0]\n",
    "    # truncate zeros\n",
    "    while key and key[-1] == 0:\n",
    "        del key[-1]\n",
    "    key = tuple(key)\n",
    "    try:\n",
    "        print('prediction:', best_model.seq2label[key])\n",
    "    except KeyError:\n",
    "        print('prediction not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-12TiO6p1S_v",
   "metadata": {
    "id": "-12TiO6p1S_v",
    "outputId": "aa8bd403-2af1-40a2-b36d-f3a9b79e08bb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc346e3cfb24c528a919b9122c3e4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8949999213218689, 'test_loss': 0.21454036235809326}\n",
      "--------------------------------------------------------------------------------\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# now with a bigger dataset\n",
    "\n",
    "best_model.start_recording_predictions()\n",
    "\n",
    "xval_test_dataset = SlicedDocuments(\n",
    "    raw_data                    = splits[xval_run][-1],  # test data\n",
    "    tokeniser                   = tokeniser,\n",
    "    fraction_for_first_sequence = 0.0,\n",
    "    max_sequence_length         = max_sequence_length,\n",
    "    second_part_as_sequence_B   = False,\n",
    "    preproc_batch_size          = 8\n",
    ")\n",
    "\n",
    "new_trainer.test(best_model, test_dataloaders = [DataLoader(\n",
    "    dataset     = xval_test_dataset,    \n",
    "    batch_size  = best_model.hparams.batch_size,\n",
    "    collate_fn  = best_model.prepare_sample,\n",
    "    num_workers = best_model.hparams.loader_workers,\n",
    ")])\n",
    "\n",
    "best_model.stop_recording_predictions()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SvumO-S_1S_v",
   "metadata": {
    "id": "SvumO-S_1S_v",
    "outputId": "47c9a67e-fca2-4c08-9067-1f2fc2c9e929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tgold\tprediction\tcorrect\n",
      "[0]\tpos\tpos\t✓\n",
      "[1]\tpos\tneg\tx\n",
      "[2]\tpos\tpos\t✓\n",
      "[3]\tpos\tpos\t✓\n",
      "[4]\tpos\tpos\t✓\n",
      "[5]\tpos\tpos\t✓\n",
      "[6]\tpos\tpos\t✓\n",
      "[7]\tpos\tpos\t✓\n",
      "[8]\tpos\tpos\t✓\n",
      "[9]\tpos\tpos\t✓\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(['index', 'gold', 'prediction', 'correct', 'text']))\n",
    "for index, item in enumerate(xval_test_dataset):\n",
    "    row = []\n",
    "    row.append('[%d]' %index)\n",
    "    row.append(item['label'])\n",
    "    input_token_ids = best_model.prepare_sample([item])[0]['input_ids']\n",
    "    key = input_token_ids.tolist()[0]\n",
    "    # truncate zeros\n",
    "    while key and key[-1] == 0:\n",
    "        del key[-1]\n",
    "    key = tuple(key)\n",
    "    try:\n",
    "        prediction = best_model.seq2label[key]\n",
    "    except KeyError:\n",
    "        prediction = 'unknown'\n",
    "    row.append(prediction)\n",
    "    if prediction == item['label']:\n",
    "        row.append('yes')\n",
    "    else:\n",
    "        row.append('no')\n",
    "    row.append(' '.join(item['parts'][0]))\n",
    "    print('\\t'.join(row))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
